---
aliases:
  - Attention Heads
  - Multi-Head
  - Attention Head
tags:
  - attention
  - hopfield
  - transformer
---
[[Attention]], [[Self-Attention]] Heads
### Regular Self-Attention Heads

> [!question]
> Correspondance between [[Self-Attention]] Heads and [[Feature Fields]] or Channels? 
> If we consider [[Transformers|Transformer]] to be an [[MPNN]] than it is equivalent to [[GAT]] - i.e. its Edge-Weights are ***Scalars***, Input-Dependent but Non-Expressive! However adding [[Attention Head|Multi-Head]] Component allows to track many ***Channels*** of the same Feature Field in parallel!
> 
> ---
> How does it improve Reasoning Capabilities of the Transformer?
### Induction Heads
![[Induction Heads|Induction Head]]

### Averaging Heads

### Sparse Heads

### MetaStable Heads
